# Birdscanner v2.2.slurm
# Last modified: fre jun 28, 2024  06:23
# Sign: JN
# Thanks to John Sundh

# TODO: [ ] Test locally
# TODO: [ ] Test on rackham using slurm
# TODO: [ ] Which are local rules?
# TODO: [ ] Can we group rules?
# TODO: [ ] Define temp() files and folders
# TODO: [ ] Add log directives: https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#log-files
# TODO: [ ] Install plast and splitfast with conda?

from glob import glob

# Input files
configfile: "config/config.yaml"
REFERENCES, = glob_wildcards("data/references/{ref}.fas")
GENOMES, = glob_wildcards("data/genomes/{genome}.gz")

# Local rules are run on the submit node on a cluster
# localrules: all
localrules: all, OO1_convert_fas, OO2_fasta_to_stockholm, OO4_cat_reference_fas, OO8_get_scaffold_ids, OO9_get_reference_ids, O10_select_scaffolds, O12_concatente_chunk_hmms, O15_concatenate_hmmer_out, 

# Helper function gather_chunk_hmm_lists
def gather_chunk_hmm_lists(wildcards):
    """
    Gather chunk lists of hmms
    """
    input = []
    ck_output_folder = checkpoints.O11_split_list_of_hmms.get(genome=wildcards.genome).output[0]
    f = f"{ck_output_folder}/{wildcards.genome}.{wildcards.n}.txt"
    with open(f, 'r') as fhin:
        for line in fhin:
            input.append(line.strip())
    return input

# Helper function gather_hmmer_out
def gather_hmmer_out(wildcards):
    """
    Gather list of .hmmer.out files (run/hmmer/genome/genome.n.hmmer.out)
    """
    input = []
    ck_output_folder = checkpoints.O11_split_list_of_hmms.get(genome=wildcards.genome).output[0]
    N = glob_wildcards(os.path.join(ck_output_folder, "{genome}.{n}.hmm")).n
    input = expand("run/hmmer/{genome}/{genome}.{n}.hmmer.out",
                   n=N, genome=wildcards.genome)
    return input

# 0. Rule for final output
rule all:
    """
    TODO: Figure out which lines that should be kept. 
    The expansion of .hmm seems undesired (but will trigger rules 2 & 3.
    """
    input:
        expand("results/genomes/{genome}", genome=GENOMES),
        expand("results/hmmer/{genome}.hmmer.out.gz", genome=GENOMES),
        expand("run/tmp/{ref}.hmm", ref=REFERENCES),
        "run/tmp/genes.done"

# 18. compress hmmer out
rule O18_compress_hmmer_out:
    """
    Compress (and keep) hmmer output.
    """
    input:
        "run/hmmer/{genome}.hmmer.out"
    output:
        "results/hmmer/{genome}.hmmer.out.gz"
    threads:
        config["pigz"]["threads"]
    conda:
        "envs/pigz.yaml"
    shell:
        """
        pigz \
          -p {threads} \
          -c {input} > {output}
        """

# 17. gather genes
rule O17_gather_genes:
    """
    Gather genes from parsed nhmmer output in genome folders.
    """
    input:
        expand("results/genomes/{genome}", genome=GENOMES)
    output:
        dir = directory("results/genes"),
        file = touch("run/tmp/genes.done")
        #file = temp(touch("run/tmp/genes.done"))
    conda:
        "envs/perl.yaml"
    shell:
        """
        perl workflow/scripts/bs2-gather-genes.pl \
          --outdir={output.dir} \
          $(find results/genomes -type d)
        """

# 16. parse_hmmer
rule O16_parse_hmmer:
    """
    Parse hmmer output.
    """
    input:
        hmmer = "run/hmmer/{genome}.hmmer.out",
        fas = "run/plast/{genome}.plast.fas"
    output:
        directory("results/genomes/{genome}")
    params:
        prefix = "{genome}",
        fastaheader = "{genome}",
        stats = config["parsehmmer"]["stats"]
    conda:
        "envs/perl.yaml"
    shell:
        """
        perl workflow/scripts/bs2-parse-nhmmer.pl \
          -f {params.fastaheader} \
          -p {params.prefix} \
          -i {input.hmmer} \
          -g {input.fas} \
          -d {output} \
          {params.stats}
        """

# 15. concatenate hmmer out
rule O15_concatenate_hmmer_out:
    """
    Concatenate run/hmmer/genome/genome.n.hmmer.out to
    run/hmmer/genome.hmmer.out
    """
    input:
        gather_hmmer_out
    output:
        "run/hmmer/{genome}.hmmer.out"
    conda:
        "envs/conda.yaml"
    shell:
        """
        cat {input} > {output}
        """

# 14. run_hmmer
rule O14_run_hmmer:
    """
    Run HMMer: the selected scaffolds against the selected hmms.
    """
    input:
        "run/tmp/{genome}.{n}.hmmpress.done",
        hmm = "run/hmmer/{genome}/{genome}.{n}.hmm",
        query = "run/plast/{genome}.plast.fas"
    output:
        "run/hmmer/{genome}.{n}.hmmer.out"
        #temp("run/hmmer/{genome}.{n}.hmmer.out")
    params:
        hmmerprog = config["type"]["hmmerprog"],
        outfmt = config["hmmer"]["outfmt"],
        chunks = config["split"]["chunks"]
    threads:
        config["hmmer"]["threads"]
    conda:
        "envs/hmmer.yaml"
    shell:
        """
        {params.hmmerprog} \
          {params.outfmt} \
          --cpu {threads} \
          --tblout {output} \
          {input.hmm} \
          {input.query} > /dev/null
        """

# 13. run_hmmpress
# TODO: which rule is creating "run/hmmer/{genome}/{genome}.{n}.hmm" (the input for this rule)?
rule O13_run_hmmpress:
    """
    Run hmmpress.
    """
    input:
        "run/hmmer/{genome}/{genome}.{n}.hmm"
    output:
        expand("run/hmmer/{{genome}}/{{genome}}.{{n}}.hmm.h3{s}",
               s=["f","i","m","p"]),
        done = "run/tmp/{genome}.{n}.hmmpress.done"
    log:
        "logs/hmmer/{genome}.{n}.log"
    conda:
        "envs/hmmer.yaml"
    shell:
        """
        hmmpress \
          {input} > {log} 2>&1 \
        touch {output.done}
        """

# 12. concatente chunk hmms
rule O12_concatente_chunk_hmms:
    """
    Concatenate lists of hmms to chunk hmms.
    The path to the files are written inside the run/hmmer/{genome}/{genome}.{n}.txt files
    """
    input:
        gather_chunk_hmm_lists
    output:
        "run/hmmer/{genome}/{genome}.{n}.hmm"
    conda:
        "envs/conda.yaml"
    shell:
        """
        cat {input} > {output}
        """

# 11. split_list_of_hmms
checkpoint O11_split_list_of_hmms:
    """
    Split the list of hmm files in chunks.
    Input: a file run/tmp/Apa.ref.ids with file paths
    Output: run/hmmer/Apa/Apa.00.txt, ..., run/hmmer/Apa/Apa.N.txt.
            N files according to int chunks.
            txt files contain paths of files.
    """
    input:
        "run/tmp/{genome}.ref.ids"
    output:
        directory("run/hmmer/{genome}/")
    conda:
        "envs/conda.yaml"
    params:
        chunks = config["split"]["chunks"]
    shell:
        """
        mkdir {output}
        n_lines_refids=$(wc -l < {input})
        split \
          -d \
          --number=l/{params.chunks} \
          --elide-empty-files \
          --additional-suffix='.txt' \
          {input} \
          "run/hmmer/{wildcards.genome}/{wildcards.genome}."
        """

# 10. select_scaffolds
rule O10_select_scaffolds:
    """
    Extract the scaffolds with best plast hits from the splitted genome fasta.
    """
    input:
        db = "run/plast/{genome}.split.fas",
        idfile = "run/tmp/{genome}.scaffolds.ids"
    output:
        "run/plast/{genome}.plast.fas"
        #temp("run/plast/{genome}.plast.fas")
    params:
        dbtype = config["type"]["dbtype"],
        outfmt = config["blastdbcmd"]["outfmt"]
    conda:
        "envs/blast.yaml"
    shell:
        """
        blastdbcmd \
          -db {input.db} \
          -dbtype {params.dbtype} \
          -entry_batch {input.idfile} \
          -outfmt {params.outfmt} \
          -out {output}
        """

# 9. get_reference_ids
rule OO9_get_reference_ids:
    """
    Get IDs (file names) for those reference gene files that have a best plast hit
    with length above minlen.
    NOTE: Room here for more filtering if needed.
    NOTE: The awk step is dependent on finding the '__', which was introduced in
    rule convert_fas, to work.
    """
    input:
        "run/plast/{genome}.plast.tab"
    output:
        "run/tmp/{genome}.ref.ids"
        #temp("run/tmp/{genome}.ref.ids")
    conda:
        "envs/conda.yaml"
    params:
        minlen = config["lengthfilter"]["min"]
    shell:
        """
        awk '$4>{params.minlen}' {input} | \
          sort -t$'\t' -k1g -k12rg | \
          awk -F $'\t' '!x[$1]++' | \
          awk -F $'__' '{{print $1 ".hmm"}}' | \
          sort -u > {output}
        """

# 8. get_scaffold_ids
rule OO8_get_scaffold_ids:
    """
    The idea is that for each species, we would expect one best hit, and we wish to
    see which of the scaffolds that most often provides the best hit.  We wish to
    search with hmms only against the scaffolds having sufficiently long hits in
    the plast search.

    Plast outfmt 1:
    query ID, subject ID, percent identities, alignment length, nb. misses,
    nb. gaps, query begin, query end, subject begin, subject end, e-value, bit score

    We will extract hits where ('alignment length' > minlen), and then we first
    sort the table on query ID, then sort the table on 'bit score' in descending
    order, and finally go down the list and keep unique 'subject ID'. Note: There
    is room here to include a scaffold only if having some bit score or e-value.

    """
    input:
        "run/plast/{genome}.plast.tab"
    output:
        "run/tmp/{genome}.scaffolds.ids"
        #temp("run/tmp/{genome}.scaffolds.ids")
    conda:
        "envs/conda.yaml"
    params:
        minlen = config["lengthfilter"]["min"]
    shell:
        """
        awk '$4>{params.minlen}' {input} | \
          sort -t$'\t' -k1,1 -k12rg | \
          awk -F $'\t' '!x[$1]++' | \
          awk -F $'\t' '{{print $2}}' | \
          sort -u > {output}
        """

# 7. run_plast
rule OO7_run_plast:
    """
    Run plast with all concatenated reference sequences as query,
    and the splitted genome fasta as data base.
    """
    input:
        db_done = "run/tmp/{genome}.makeblastdb.done",
        query = "run/tmp/reference.fas"
    output:
        "run/plast/{genome}.plast.tab"
        #temp("run/plast/{genome}.plast.tab")
    conda:
        "envs/conda.yaml"
    threads:
        config["plast"]["threads"]
    params:
        maxhitperquery = config["plast"]["maxhitperquery"],
        bargraph = config["plast"]["bargraph"],
        plastprog = config["type"]["plastprog"]
    shell:
        """
        plast \
          -p {params.plastprog} \
          -i {input.query} \
          -d run/plast/{wildcards.genome}.split.fas \
          -o {output} \
          -a {threads} \
          -max-hit-per-query {params.maxhitperquery} \
          {params.bargraph}
        """

# 6. make_genome_plast_db
rule OO6_make_genome_plast_db:
    """
    Make plast database.
    """
    input:
        "run/plast/{genome}.split.fas",
    output:
        touch("run/tmp/{genome}.makeblastdb.done")
        #temp(touch("run/tmp/{genome}.makeblastdb.done"))
    params:
        dbtype = config["type"]["dbtype"],
        parse_seqids = config["makeblastdb"]["parseseqids"]
    conda:
        "envs/blast.yaml"
    shell:
        """
        makeblastdb \
          -in {input} \
          -dbtype {params.dbtype} \
          {params.parse_seqids}
        """

# 5. split_genome_fasta
rule OO5_split_genome_fasta:
    """
    Split fasta sequences longer than length=100,000.
    Note that 100,000 is the maximum length for plast.
    """
    input:
        "data/genomes/{genome}.gz"
    output:
        "run/plast/{genome}.split.fas"
        #temp("run/plast/{genome}.split.fas")
    conda:
        "envs/conda.yaml"
    threads:
        config["pigz"]["threads"]
    params:
        length = config["splitfast"]["max"]
    conda:
        "envs/pigz.yaml"
    shell:
        """
        splitfast \
          -m {params.length} \
          <(pigz -p {threads} -d -c {input}) > {output}
        """

# 4. cat_reference_fas
rule OO4_cat_reference_fas:
    """
    Concatenate all reference fasta files to one (reference.fas).
    """
    input:
        expand("run/tmp/{ref}.fasta", ref=REFERENCES)
    output:
        "run/tmp/reference.fas"
        #temp("run/tmp/reference.fas")
    conda:
        "envs/conda.yaml"
    shell:
        """
        find \
          run/tmp \
          -type f \
          -name '*.fasta' \
          -exec cat {{}} \+ > {output}
        """

# 3. create_hmms
rule OO3_create_hmms:
    """
    Create hmms from Stockholm format.
    """
    input:
        "run/tmp/{ref}.sto"
    output:
        "run/tmp/{ref}.hmm"
        #temp("run/tmp/{ref}.hmm")
    group:
        "convert"
    threads:
        config["hmmbuild"]["threads"]
    params:
        hmmbuildtype = config["type"]["hmmbuildtype"]
    conda:
        "envs/hmmer.yaml"
    shell:
        """
        hmmbuild \
          {params.hmmbuildtype} \
          --cpu {threads} \
          {output} \
          {input} > /dev/null 2>&1

        """

# 2. fasta_to_stockholm
rule OO2_fasta_to_stockholm:
    """
    Fasta to Stockholm MSA conversion.
    """
    input:
        "data/references/{ref}.fas"
    output:
        "run/tmp/{ref}.sto"
        #temp("run/tmp/{ref}.sto")
    group:
        "convert"
    conda:
        "envs/perl.yaml"
    shell:
        """
        perl workflow/scripts/bs2-fas-to-sto.pl \
          {input} > {output}
        """

# 1. Convert fasta
rule OO1_convert_fas:
    """
    Relabel fasta headers to '>filename__i', where i is an iterator.
    The '__' is important and used later in rule get_reference_ids.
    """
    input:
        "data/references/{ref}.fas"
    output:
        "run/tmp/{ref}.fasta"
        #temp("run/tmp/{ref}.fasta")
    group:
        "convert"
    conda:
        "envs/conda.yaml"
    shell:
        """
        awk -v a=$(basename {input} .fas) \
          '/>/{{$0=">"a"__"++i}}1' \
          {input} > {output}
        """

# Helper rule clean
rule clean:
    """
    Remove run directory.
    """
    shell:
        """
        rm -rf run/
        """

# Helper rule distclean
rule distclean:
    """
    Remove run and results directories.
    """
    shell:
        """
        rm -rf run/ results/
        """

# On success, remove files
onsuccess:
    """
    Remove run directory on succesful finish.
    """
    shell("rm -r run ; find ./results -type f -name .snakemake_timestamp -exec rm {{}} +")

